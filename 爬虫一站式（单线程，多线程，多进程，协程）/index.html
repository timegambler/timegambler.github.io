<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="爬虫一站式（单线程，多线程，多进程，协程）I、 单线程爬虫1、全部代码 from bs4 import BeautifulSoup import urllib3 from lxml import etree import  os  #关闭无认证警告 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  #爬取网站h">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫一站式（单线程，多线程，多进程，协程）">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;%E7%88%AC%E8%99%AB%E4%B8%80%E7%AB%99%E5%BC%8F%EF%BC%88%E5%8D%95%E7%BA%BF%E7%A8%8B%EF%BC%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%EF%BC%8C%E5%8D%8F%E7%A8%8B%EF%BC%89&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="爬虫一站式（单线程，多线程，多进程，协程）I、 单线程爬虫1、全部代码 from bs4 import BeautifulSoup import urllib3 from lxml import etree import  os  #关闭无认证警告 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  #爬取网站h">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-12-22T14:00:25.684Z">
<meta property="article:modified_time" content="2019-12-22T14:00:25.684Z">
<meta property="article:author" content="timegambler">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/爬虫一站式（单线程，多线程，多进程，协程）/"/>





  <title>爬虫一站式（单线程，多线程，多进程，协程） | Hexo</title>
  








<meta name="generator" content="Hexo 4.1.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/%E7%88%AC%E8%99%AB%E4%B8%80%E7%AB%99%E5%BC%8F%EF%BC%88%E5%8D%95%E7%BA%BF%E7%A8%8B%EF%BC%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%EF%BC%8C%E5%8D%8F%E7%A8%8B%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="timegambler">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">爬虫一站式（单线程，多线程，多进程，协程）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-12-22T22:00:25+08:00">
                2019-12-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="爬虫一站式（单线程，多线程，多进程，协程）"><a href="#爬虫一站式（单线程，多线程，多进程，协程）" class="headerlink" title="爬虫一站式（单线程，多线程，多进程，协程）"></a>爬虫一站式（单线程，多线程，多进程，协程）</h1><p>I、 单线程爬虫<br>1、全部代码</p>
<pre><code>from bs4 import BeautifulSoup
import urllib3
from lxml import etree
import  os

#关闭无认证警告
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

#爬取网站host
host =&apos;https://www.ethanallen.com&apos;

#测试接口
home_url = &apos;https://www.ethanallen.com/en_US/shop-decor-decorative-accents-baskets&apos;

#建立连接，返回数据
def urllib3_get(url):
    http = urllib3.PoolManager()
    r = http.request(&apos;GET&apos;,url)
    html = r.data.decode(&apos;utf-8&apos;)
    return  html

#分析url，返回相关文件夹路径
def analysis_url(url):
    parlist = url.split(&apos;-&apos;)
    if parlist :
        url_dirname = &apos;_&apos;.join(parlist[1:])
    return url_dirname


def get_path(url):
    return  &apos;data/&apos;+analysis_url(url)

#去除标题中的非法字符，否则会引起路径错误
def deal_illegl_title(title):
    title = title.strip()
    title = title.split(&apos;/&apos;)[0]
    title = title.replace(&apos;&quot;&apos;,&apos;&apos;)
    title = title.replace(&apos;\&apos;&apos;,&apos;&apos;)
    title = title.replace(&apos;*&apos;, &apos;&apos;)
    # title = title.replace(&apos; &apos;,&apos;&apos;)
    return  title
# print(deal_illegl_title(&apos;5&quot; x 7&quot; Black Croc Frame&apos;))


#获取单个文件中的全部链接
def get_file_product_link(path):
    # print(path)
    with open(path,&apos;r&apos;) as fout:
        str_link =fout.read()
        # print(str_link)
        return str_link.split(&apos;\n&apos;)

#保存所有解析失败的url,用于后期分析
def save_fail_url(url):
    with open(&apos;bad_url.txt&apos;, &apos;a&apos;) as fin:
        fin.write(url)

#清除空文件夹或者无用文件夹
def clear_blockdir():
    category_dir = os.listdir(&apos;data&apos;)
    for dir in category_dir:
        for file in os.listdir(dir):
            with open(file,&apos;r&apos;) as fp:
                if not fp.read():
                    print(file,&apos;文件为无用或者错误文件，删除该文件&apos;)
                    os.remove(file)

#获得3级目录，有的是直接商品展示，有的还需要进行二次跳转
def get_3_link(url):
    print(&apos;开始解析三级目录.....&apos;)
    html = urllib3_get(url)
    soup = BeautifulSoup(html, &quot;lxml&quot;)
    list_data = soup.find_all(&apos;a&apos;,class_ = &apos;level-3-link mobile-menu-row&apos;)
    href_list = []
    for data in list_data:
        href_list.append(data[&apos;href&apos;]+&apos;\n&apos;)
    with open(&apos;layer_3.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)as fp:
        fp.writelines(href_list)
    print(&apos;解析完成....&apos;)

#爬取四级目录，并且获取商品展示
def get_4_link(url):
    print(&apos;开始解析四级目录。。。&apos;)
    html = urllib3_get(url)
    soup = BeautifulSoup(html, &quot;lxml&quot;)
    list_data = soup.find_all(&apos;a&apos;,class_ = &apos;level-4-link mobile-menu-row&apos;)
    href_list = []
    for data in list_data:
        href_list.append(data[&apos;href&apos;]+&apos;\n&apos;)
    print(&apos;解析完成....&apos;)
    with open(&apos;layer_4.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)as fp:
        fp.writelines(href_list)


#直接读取文件链接，生成目录,并且商品信息获取链接放入对应的文件product.txt中
def get_link_to_dir_txt(path):
    print(&apos;开始生成文件夹和product.txt文件...&apos;)
    url_list = get_file_product_link(path)
    for url in url_list:
        try:
            path = get_path(url)
            if not os.path.exists(path):
                print(&apos;生成文件夹:&apos;,path)
                os.makedirs(path)
        # 提取商品链接,直接每页最高500
            html = urllib3_get(url+&apos;?sz=500&amp;start=0&apos;)
            selector = etree.HTML(html)
            content = selector.xpath(&apos;/html/body/div/div/div/div/div/div/ul/li/div/div/a/@href&apos;)
            with open(path + &apos;\\product.txt&apos;, &apos;w&apos;) as fp:
                print(&apos;生成文件：&apos;,path+&apos;\\product.txt&apos;)
                for line in content:
                    if host in line:
                        fp.write(line+&apos;\n&apos;)
                    else:
                        fp.write(&apos;https://www.ethanallen.com&apos; + line + &apos;\n&apos;)
            print(&apos;商品目录写入product.txt成功...&apos;)
        except:
            print(url,&apos;未成功，该链接失败&apos;)
            save_fail_url(url)
        # print(&apos;生成结束&apos;)



#获取所有文件夹中的商品链接,以及其所在的目录,返回一个字典，保护文件路径及链接列表
def get_dir_product_link():
    category_dir  =os.listdir(&apos;data&apos;)
    product_dict ={}
    link_list = []
    for dir in category_dir:
        if &apos;txt&apos; in dir:
            continue
        dirpath = &apos;data&apos;+&apos;\\&apos;+dir
        link_list1=get_file_product_link(dirpath + &apos;\\product.txt&apos;)
        product_dict={&apos;dirpath&apos;:dirpath,&apos;link&apos;:link_list1}
        link_list.append(product_dict)
    return link_list
# print(len(get_dir_product_link()))


#开始读取数据
def get_data(link_list):
    for link_json in link_list:
        for url in link_json[&apos;link&apos;]:
            if not url:
                continue
            # try:
            url=url.strip()
            html = urllib3_get(url)
            soup = BeautifulSoup(html,&apos;lxml&apos;)

            title = soup.find(&apos;h1&apos;,class_=&quot;product-name&quot;)
            if not title:
                continue
            title = deal_illegl_title(title.text)
            print(&apos;当前访问url：&apos;,url)
            print(&apos;商品名称:&apos;,title)
            page_soup  = soup.find(&apos;div&apos;,class_=&quot;toggle-content&quot;)
            page_brife = page_soup.find_all(&apos;p&apos;)
            page_list = page_soup.find_all(&apos;li&apos;)
            brief_p = []
            for p  in  page_brife:
                if not p:
                    continue
                brief_p.append(p.text)
            list_li = []
            for ll in page_list:
                if not ll:
                    continue
                list_li.append(ll.text+&apos;\n&apos;)
            print(&apos;商品介绍：&apos;)
            print(str(brief_p))
            print(str(list_li))
            with open(link_json[&apos;dirpath&apos;]+&apos;\\&apos;+title+&apos;.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:
                fp.writelines(brief_p)
                fp.writelines(list_li)


def controller():
    #开始执行
    #第一步，先执行获取三四级目录链接，将链接写入layer3.txt和layer4.txt中
    # get_3_link(home_url)
    # get_4_link(home_url)

    #第二步，创建目录并在每个目录下生成相关产品product.txt文件
    get_link_to_dir_txt(&apos;layer_4.txt&apos;)
    get_link_to_dir_txt(&apos;layer_3.txt&apos;)

    #第三步，获取链接开始将所有信息生成并导入到txt文件中
    link_list = get_dir_product_link()
    get_data(link_list)


if __name__ == &apos;__main__&apos;:
    controller()</code></pre><p>2、代码解析</p>
<p>II、多线程爬虫<br>1、全部代码</p>
<pre><code>from bs4 import BeautifulSoup
import urllib3
from lxml import etree
import  os
import threading
import queue

#关闭无认证警告
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

#爬取网站host
host =&apos;https://www.ethanallen.com&apos;

#测试接口
home_url = &apos;https://www.ethanallen.com/en_US/shop-decor-decorative-accents-baskets&apos;

#建立连接，返回数据
def urllib3_get(url):
    http = urllib3.PoolManager()
    r = http.request(&apos;GET&apos;,url)
    html = r.data.decode(&apos;utf-8&apos;)
    return  html

#分析url，返回相关文件夹路径
def analysis_url(url):
    parlist = url.split(&apos;-&apos;)
    if parlist :
        url_dirname = &apos;_&apos;.join(parlist[1:])
    return url_dirname


def get_path(url):
    return  &apos;data/&apos;+analysis_url(url)

#去除标题中的非法字符，否则会引起路径错误
def deal_illegl_title(title):
    title = title.strip()
    title = title.split(&apos;/&apos;)[0]
    title = title.replace(&apos;&quot;&apos;,&apos;&apos;)
    title = title.replace(&apos;\&apos;&apos;,&apos;&apos;)
    title = title.replace(&apos;*&apos;, &apos;&apos;)
    # title = title.replace(&apos; &apos;,&apos;&apos;)
    return  title
# print(deal_illegl_title(&apos;5&quot; x 7&quot; Black Croc Frame&apos;))


#获取单个文件中的全部链接
def get_file_product_link(path):
    # print(path)
    with open(path,&apos;r&apos;) as fout:
        str_link =fout.read()
        # print(str_link)
        return str_link.split(&apos;\n&apos;)

#保存所有解析失败的url,用于后期分析
def save_fail_url(url):
    with open(&apos;bad_url.txt&apos;, &apos;a&apos;) as fin:
        fin.write(url)

#清除空文件夹或者无用文件夹
def clear_blockdir():
    category_dir = os.listdir(&apos;data&apos;)
    for dir in category_dir:
        for file in os.listdir(dir):
            with open(file,&apos;r&apos;) as fp:
                if not fp.read():
                    print(file,&apos;文件为无用或者错误文件，删除该文件&apos;)
                    os.remove(file)

#获得3级目录，有的是直接商品展示，有的还需要进行二次跳转
def get_3_link(url):
    print(&apos;开始解析三级目录.....&apos;)
    html = urllib3_get(url)
    soup = BeautifulSoup(html, &quot;lxml&quot;)
    list_data = soup.find_all(&apos;a&apos;,class_ = &apos;level-3-link mobile-menu-row&apos;)
    href_list = []
    for data in list_data:
        href_list.append(data[&apos;href&apos;]+&apos;\n&apos;)
    with open(&apos;layer_3.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)as fp:
        fp.writelines(href_list)
    print(&apos;解析完成....&apos;)

#爬取四级目录，并且获取商品展示
def get_4_link(url):
    print(&apos;开始解析四级目录。。。&apos;)
    html = urllib3_get(url)
    soup = BeautifulSoup(html, &quot;lxml&quot;)
    list_data = soup.find_all(&apos;a&apos;,class_ = &apos;level-4-link mobile-menu-row&apos;)
    href_list = []
    for data in list_data:
        href_list.append(data[&apos;href&apos;]+&apos;\n&apos;)
    print(&apos;解析完成....&apos;)
    with open(&apos;layer_4.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)as fp:
        fp.writelines(href_list)


#直接读取文件链接，生成目录,并且商品信息获取链接放入对应的文件product.txt中
def get_link_to_dir_txt(path):
    print(&apos;开始生成文件夹和product.txt文件...&apos;)
    url_list = get_file_product_link(path)
    for url in url_list:
        try:
            path = get_path(url)
            if not os.path.exists(path):
                print(&apos;生成文件夹:&apos;,path)
                os.makedirs(path)
        # 提取商品链接,直接每页最高500
            html = urllib3_get(url+&apos;?sz=500&amp;start=0&apos;)
            selector = etree.HTML(html)
            content = selector.xpath(&apos;/html/body/div/div/div/div/div/div/ul/li/div/div/a/@href&apos;)
            with open(path + &apos;\\product.txt&apos;, &apos;w&apos;) as fp:
                print(&apos;生成文件：&apos;,path+&apos;\\product.txt&apos;)
                for line in content:
                    if host in line:
                        fp.write(line+&apos;\n&apos;)
                    else:
                        fp.write(&apos;https://www.ethanallen.com&apos; + line + &apos;\n&apos;)
            print(&apos;商品目录写入product.txt成功...&apos;)
        except:
            print(url,&apos;未成功，该链接失败&apos;)
            save_fail_url(url)
        # print(&apos;生成结束&apos;)



#获取所有文件夹中的商品链接,以及其所在的目录,返回一个字典，保护文件路径及链接列表
def get_dir_product_link():
    category_dir  =os.listdir(&apos;data&apos;)
    product_dict ={}
    link_queue = queue.Queue(maxsize=0)
    for dir in category_dir:
        if &apos;txt&apos; in dir:
            continue
        dirpath = &apos;data&apos;+&apos;\\&apos;+dir
        link_list1=get_file_product_link(dirpath + &apos;\\product.txt&apos;)
        product_dict={&apos;dirpath&apos;:dirpath,&apos;link&apos;:link_list1}
        # link_list.append(product_dict)
        link_queue.put(product_dict)
    return link_queue
# print(len(get_dir_product_link()))


#开始读取数据
def get_p_data(link_list):
    for link_json in link_list:
        for url in link_json[&apos;link&apos;]:
            if not url:
                continue
            # try:
            url=url.strip()
            html = urllib3_get(url)
            soup = BeautifulSoup(html,&apos;lxml&apos;)

            title = soup.find(&apos;h1&apos;,class_=&quot;product-name&quot;)
            if not title:
                continue
            title = deal_illegl_title(title.text)
            print(&apos;当前访问url：&apos;,url)
            print(&apos;商品名称:&apos;,title)
            page_soup  = soup.find(&apos;div&apos;,class_=&quot;toggle-content&quot;)
            page_brife = page_soup.find_all(&apos;p&apos;)
            page_list = page_soup.find_all(&apos;li&apos;)
            brief_p = []
            for p  in  page_brife:
                if not p:
                    continue
                brief_p.append(p.text)
            list_li = []
            for ll in page_list:
                if not ll:
                    continue
                list_li.append(ll.text+&apos;\n&apos;)
            print(&apos;商品介绍：&apos;)
            print(str(brief_p))
            print(str(list_li))
            with open(link_json[&apos;dirpath&apos;]+&apos;\\&apos;+title+&apos;.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:
                fp.writelines(brief_p)
                fp.writelines(list_li)


def controller():
    #开始执行
    #第一步，先执行获取三四级目录链接，将链接写入layer3.txt和layer4.txt中
    get_3_link(home_url)
    get_4_link(home_url)

    #第二步，创建目录并在每个目录下生成相关产品product.txt文件
    get_link_to_dir_txt(&apos;layer_4.txt&apos;)
    get_link_to_dir_txt(&apos;layer_3.txt&apos;)

    #第三步，获取链接开始将所有信息生成并导入到txt文件中,使用多线程
    link_queue = get_dir_product_link()
    while not link_queue.empty():
        threads_list = []
        #设置进程数目
        threads_sum = 20
        for count in range(threads_sum):
            thread_ = MyThread(link_queue)
            threads_list.append(thread_)
        for t in threads_list:
            t.start()
        for t in threads_list:
            t.join()


    # get_data(link_list)


class MyThread(threading.Thread):
    def __init__(self, q):
        threading.Thread.__init__(self)
        self.q = q

    #调用get_index()
    def run(self) -&gt; None:
        self.get_data()

    #拿到网址后获取所需要的数据并存入全局变量data_list中
    def get_data(self):
        link_json = self.q.get()
        try:
            for url in link_json[&apos;link&apos;]:
                if not url:
                    continue
                # try:
                url = url.strip()
                html = urllib3_get(url)
                soup = BeautifulSoup(html, &apos;lxml&apos;)

                title = soup.find(&apos;h1&apos;, class_=&quot;product-name&quot;)
                if not title:
                    continue
                title = deal_illegl_title(title.text)
                print(&apos;当前访问url：&apos;, url)
                print(&apos;商品名称:&apos;, title)
                page_soup = soup.find(&apos;div&apos;, class_=&quot;toggle-content&quot;)
                page_brife = page_soup.find_all(&apos;p&apos;)
                page_list = page_soup.find_all(&apos;li&apos;)
                brief_p = []
                for p in page_brife:
                    if not p:
                        continue
                    brief_p.append(p.text)
                list_li = []
                for ll in page_list:
                    if not ll:
                        continue
                    list_li.append(ll.text + &apos;\n&apos;)
                print(&apos;商品介绍：&apos;)
                print(str(brief_p))
                print(str(list_li))
                with open(link_json[&apos;dirpath&apos;] + &apos;\\&apos; + title + &apos;.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as fp:
                    fp.writelines(brief_p)
                    fp.writelines(list_li)

        except Exception as e:
            # 如果访问超时就打印错误信息，并将该条url放入队列，防止出错的url没有爬取
            self.q.put(url)
            print(e)



if __name__ == &apos;__main__&apos;:
    controller()</code></pre><p>2、代码解析</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/%E5%BF%AB%E6%89%8B%E9%80%86%E5%90%91%E8%AE%B0%E5%BD%95/" rel="next" title="快手逆向记录">
                <i class="fa fa-chevron-left"></i> 快手逆向记录
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/xpath%E6%8E%92%E5%9D%91%E8%AE%B0/" rel="prev" title="xpath排坑记">
                xpath排坑记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">timegambler</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">89</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#爬虫一站式（单线程，多线程，多进程，协程）"><span class="nav-number">1.</span> <span class="nav-text">爬虫一站式（单线程，多线程，多进程，协程）</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">timegambler</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
